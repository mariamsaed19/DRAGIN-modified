INFO:faiss.loader:Loading faiss.
INFO:faiss.loader:Successfully loaded faiss.
/root/miniconda3/envs/dragin/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Some weights of the model checkpoint at /root/.camel_tools/data/disambig_bert_unfactored/msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:__main__:Namespace(model_name_or_path='/scratch/mariam.saeed/side/advanced-nlp/models/aya-8bit', method='dragin', dataset='arabicaqa', data_path='/scratch/mariam.saeed/side/advanced-nlp/ArabicaQA', fewshot=0, sample=50, shuffle=False, generate_max_length=1024, query_formulation='real_words', retrieve_keep_top_k=35, output_dir='../result/aya-chat-arabicaqa-dragin-AraDPR', retriever='AraDPR', es_index_name='wiki_ar', retrieve_topk=3, hallucination_threshold=1.6, check_real_words=True, use_counter=True, config_path='../config/aya-expanse-quantized/ArabicaQA/AraDPR.json')
INFO:__main__:output dir: ../result/aya-chat-arabicaqa-dragin-AraDPR/4
INFO:data:Loading ArabicaQA from /scratch/mariam.saeed/side/advanced-nlp/ArabicaQA
  0%|          | 0/12715 [00:00<?, ?it/s]100%|██████████| 12715/12715 [00:00<00:00, 2218963.77it/s]
Map:   0%|          | 0/12715 [00:00<?, ? examples/s]Map:  27%|██▋       | 3418/12715 [00:00<00:00, 34055.64 examples/s]Map:  55%|█████▌    | 7000/12715 [00:00<00:00, 34723.00 examples/s]Map:  87%|████████▋ | 11000/12715 [00:00<00:00, 35415.83 examples/s]Map: 100%|██████████| 12715/12715 [00:00<00:00, 35320.06 examples/s]
INFO:generate:Loading model from /scratch/mariam.saeed/side/advanced-nlp/models/aya-8bit
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]
INFO:__main__:start inference
  0%|          | 0/50 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
CohereModel is using CohereSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  2%|▏         | 1/50 [00:55<45:15, 55.41s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  4%|▍         | 2/50 [02:11<54:02, 67.56s/it]  6%|▌         | 3/50 [03:47<1:03:09, 80.62s/it]  8%|▊         | 4/50 [05:12<1:02:59, 82.16s/it] 10%|█         | 5/50 [06:18<57:15, 76.33s/it]   12%|█▏        | 6/50 [06:44<43:35, 59.43s/it] 14%|█▍        | 7/50 [08:30<53:30, 74.66s/it] 16%|█▌        | 8/50 [09:18<46:11, 65.99s/it] 18%|█▊        | 9/50 [09:38<35:23, 51.79s/it] 20%|██        | 10/50 [09:42<24:43, 37.08s/it] 22%|██▏       | 11/50 [09:54<19:00, 29.24s/it] 24%|██▍       | 12/50 [09:59<13:56, 22.01s/it] 26%|██▌       | 13/50 [10:12<11:51, 19.24s/it] 28%|██▊       | 14/50 [10:24<10:13, 17.04s/it] 30%|███       | 15/50 [10:59<13:00, 22.29s/it] 32%|███▏      | 16/50 [13:06<30:32, 53.90s/it] 34%|███▍      | 17/50 [13:36<25:42, 46.73s/it] 36%|███▌      | 18/50 [14:06<22:14, 41.69s/it] 38%|███▊      | 19/50 [14:31<18:55, 36.63s/it] 40%|████      | 20/50 [14:33<13:07, 26.25s/it] 42%|████▏     | 21/50 [14:52<11:36, 24.02s/it] 44%|████▍     | 22/50 [15:14<11:01, 23.61s/it] 46%|████▌     | 23/50 [15:38<10:41, 23.75s/it] 48%|████▊     | 24/50 [16:07<10:57, 25.27s/it] 50%|█████     | 25/50 [16:42<11:43, 28.15s/it] 52%|█████▏    | 26/50 [17:03<10:21, 25.88s/it] 54%|█████▍    | 27/50 [17:53<12:40, 33.09s/it] 56%|█████▌    | 28/50 [19:06<16:35, 45.26s/it] 58%|█████▊    | 29/50 [19:59<16:34, 47.37s/it] 60%|██████    | 30/50 [21:27<19:52, 59.64s/it] 62%|██████▏   | 31/50 [21:47<15:07, 47.75s/it] 64%|██████▍   | 32/50 [22:09<12:01, 40.07s/it] 66%|██████▌   | 33/50 [26:32<30:16, 106.84s/it] 68%|██████▊   | 34/50 [27:08<22:49, 85.58s/it]  70%|███████   | 35/50 [27:22<16:03, 64.23s/it] 72%|███████▏  | 36/50 [28:14<14:09, 60.67s/it] 74%|███████▍  | 37/50 [28:24<09:50, 45.46s/it] 76%|███████▌  | 38/50 [28:52<08:02, 40.24s/it] 78%|███████▊  | 39/50 [29:58<08:45, 47.79s/it] 80%|████████  | 40/50 [30:08<06:04, 36.45s/it] 82%|████████▏ | 41/50 [30:24<04:32, 30.30s/it] 84%|████████▍ | 42/50 [30:59<04:14, 31.75s/it] 86%|████████▌ | 43/50 [32:40<06:07, 52.47s/it] 88%|████████▊ | 44/50 [32:55<04:08, 41.42s/it] 90%|█████████ | 45/50 [33:23<03:05, 37.20s/it] 92%|█████████▏| 46/50 [33:59<02:27, 36.95s/it] 94%|█████████▍| 47/50 [34:04<01:22, 27.45s/it] 96%|█████████▌| 48/50 [34:17<00:45, 22.91s/it] 98%|█████████▊| 49/50 [35:11<00:32, 32.50s/it]100%|██████████| 50/50 [35:27<00:00, 27.46s/it]100%|██████████| 50/50 [35:27<00:00, 42.55s/it]
