INFO:faiss.loader:Loading faiss with AVX512 support.
INFO:faiss.loader:Successfully loaded faiss with AVX512 support.
/root/miniconda3/envs/dragin/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Some weights of the model checkpoint at /root/.camel_tools/data/disambig_bert_unfactored/msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:__main__:Namespace(model_name_or_path='/scratch/mariam.saeed/side/advanced-nlp/models/aya-8bit', method='dragin', dataset='arabicaqa', data_path='/scratch/mariam.saeed/side/advanced-nlp/ArabicaQA', fewshot=0, sample=50, shuffle=False, generate_max_length=1024, query_formulation='real_words', retrieve_keep_top_k=35, output_dir='../result/aya-chat-arabicaqa-dragin-AraDPR', retriever='AraDPR', es_index_name='wiki_ar', retrieve_topk=3, hallucination_threshold=1.6, check_real_words=True, use_counter=True, config_path='../config/aya-expanse-quantized/ArabicaQA/AraDPR.json')
INFO:__main__:output dir: ../result/aya-chat-arabicaqa-dragin-AraDPR/6
INFO:data:Loading ArabicaQA from /scratch/mariam.saeed/side/advanced-nlp/ArabicaQA
  0%|          | 0/12715 [00:00<?, ?it/s]100%|██████████| 12715/12715 [00:00<00:00, 2378069.00it/s]
Map:   0%|          | 0/12715 [00:00<?, ? examples/s]Map:  30%|██▉       | 3760/12715 [00:00<00:00, 37465.25 examples/s]Map:  59%|█████▉    | 7538/12715 [00:00<00:00, 37642.92 examples/s]Map:  89%|████████▉ | 11314/12715 [00:00<00:00, 37694.83 examples/s]Map: 100%|██████████| 12715/12715 [00:00<00:00, 37490.13 examples/s]
INFO:generate:Loading model from /scratch/mariam.saeed/side/advanced-nlp/models/aya-8bit
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.66s/it]
INFO:__main__:start inference
  0%|          | 0/50 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  2%|▏         | 1/50 [04:22<3:33:58, 262.00s/it]  4%|▍         | 2/50 [05:26<1:56:50, 146.05s/it]  6%|▌         | 3/50 [05:59<1:13:42, 94.10s/it]   8%|▊         | 4/50 [07:17<1:07:18, 87.80s/it] 10%|█         | 5/50 [08:21<59:21, 79.14s/it]   12%|█▏        | 6/50 [09:23<53:52, 73.46s/it] 14%|█▍        | 7/50 [10:51<56:01, 78.19s/it] 16%|█▌        | 8/50 [11:12<42:03, 60.08s/it] 18%|█▊        | 9/50 [11:40<34:06, 49.92s/it] 20%|██        | 10/50 [11:44<23:49, 35.74s/it] 22%|██▏       | 11/50 [11:56<18:28, 28.42s/it] 24%|██▍       | 12/50 [12:00<13:27, 21.25s/it] 26%|██▌       | 13/50 [12:11<11:02, 17.92s/it] 28%|██▊       | 14/50 [12:19<09:03, 15.10s/it] 30%|███       | 15/50 [12:41<09:54, 16.99s/it] 32%|███▏      | 16/50 [14:16<22:55, 40.47s/it] 34%|███▍      | 17/50 [14:38<19:12, 34.94s/it] 36%|███▌      | 18/50 [14:50<14:57, 28.04s/it] 38%|███▊      | 19/50 [15:05<12:31, 24.23s/it] 40%|████      | 20/50 [15:07<08:46, 17.54s/it] 42%|████▏     | 21/50 [15:26<08:40, 17.96s/it] 44%|████▍     | 22/50 [15:39<07:37, 16.36s/it] 46%|████▌     | 23/50 [16:02<08:19, 18.49s/it] 48%|████▊     | 24/50 [16:25<08:34, 19.78s/it] 50%|█████     | 25/50 [16:32<06:36, 15.87s/it] 52%|█████▏    | 26/50 [16:41<05:37, 14.07s/it] 54%|█████▍    | 27/50 [17:22<08:24, 21.92s/it] 56%|█████▌    | 28/50 [18:20<12:00, 32.77s/it] 58%|█████▊    | 29/50 [19:05<12:47, 36.56s/it] 60%|██████    | 30/50 [20:04<14:26, 43.31s/it] 62%|██████▏   | 31/50 [20:22<11:19, 35.76s/it] 64%|██████▍   | 32/50 [20:35<08:40, 28.91s/it] 66%|██████▌   | 33/50 [21:00<07:50, 27.65s/it] 68%|██████▊   | 34/50 [21:18<06:34, 24.67s/it] 70%|███████   | 35/50 [21:29<05:07, 20.50s/it] 72%|███████▏  | 36/50 [22:12<06:25, 27.53s/it] 74%|███████▍  | 37/50 [22:22<04:49, 22.27s/it] 76%|███████▌  | 38/50 [22:46<04:32, 22.69s/it] 78%|███████▊  | 39/50 [23:11<04:15, 23.23s/it] 80%|████████  | 40/50 [23:21<03:14, 19.44s/it] 82%|████████▏ | 41/50 [23:35<02:40, 17.88s/it] 84%|████████▍ | 42/50 [24:00<02:39, 19.93s/it] 86%|████████▌ | 43/50 [25:26<04:38, 39.72s/it] 88%|████████▊ | 44/50 [26:13<04:11, 41.97s/it] 90%|█████████ | 45/50 [26:26<02:45, 33.15s/it] 92%|█████████▏| 46/50 [26:56<02:09, 32.38s/it] 94%|█████████▍| 47/50 [27:02<01:13, 24.35s/it] 96%|█████████▌| 48/50 [27:15<00:42, 21.04s/it] 98%|█████████▊| 49/50 [27:32<00:19, 19.67s/it]100%|██████████| 50/50 [27:51<00:00, 19.47s/it]100%|██████████| 50/50 [27:51<00:00, 33.43s/it]
